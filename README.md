# MixMA-LMM: MixedFrame Memory-Augmented Large Multimodal Model for Long-Term Video Understanding

## О проекте

Это форк репозитория [MA-LMM](https://github.com/boheumd/MA-LMM), в котором были внесены некоторые изменения и модификации.

MA-LMM — это модель, предназначенная для работы с многомодальными данными и выполнения задач долгосрочного понимания видео. Она использует механизм памяти, чтобы сохранять и использовать
информацию из прошлого для решения текущих задач.

## О модификации

Основая проблема интерпретации многомодальных видео заключается в большом количестве фреймов, которые нужно вытягиватьв из видео.

Как правило, используют равномерное распределение индексов фреймов через np.linspace.
В случае, если длительность видео большое, то существует риск некачественного сэмплирования фреймов, при котором часть семантики исходного видео будет утеряна.

ActionShot — технология захвата объекта в действии и отображения его на одном изображении с несколькими последовательными появлениями объекта. Такая технология позволяет из нескольких кадров формировать единый кадр с изображением действия объектов, при котором, например, можно выловить семантическое действие движения

MixMA-LMM — это формат модификации MA-LMM, при котором используется baseline ActionShot'a — наложение множества фреймов между собой через имитацию большой выдержки (функция cv2.addWeighted). Алгоритм учитывает как разницу между кадрами по пороговому значению (threshold), так и максимальное время выполнения действия в рамках объектива.

Данный репозиторий используется для хакатона Цифровой прорыв, в дальнейшем будет развиваться для исследования гипотез касательно эффективности подобного метода.

## Установка

1. Клонируйте репозиторий на свою машину:
   ```
   git clone https://github.com/your-username/your-fork.git
   ```

2. Перейдите в каталог проекта:
   ```
   cd your-fork
   ```

3. Установите зависимости:
   ```
   pip install -r requirements.txt
   ```

## Использование

1. Загрузите веса модели:
   ```
   bash run_scripts/${dataset}/train.sh
   ```

2. Проведите тестирование модели:
   ```
   bash run_scripts/${dataset}/test.sh ${checkpoint_path}
   ```

3. Выполните другие задачи, которые вы хотите выполнить с помощью модели.

## Ссылки

- [Оригинальный репозиторий MA-LMM](https://github.com/boheumd/MA-LMM)
- [Публикация о модели MA-LMM](https://arxiv.org/abs/2404.05726)

## Лицензия

Данная модель лицензирована под [BSD-3-Clause](https://opensource.org/licenses/BSD-3-Clause). Полный текст лицензии можно найти в файле `LICENSE` в корневом каталоге репозитория.
